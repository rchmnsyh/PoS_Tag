{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w0tqHrvIwsxL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "from itertools import permutations\n",
    "import re\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rjfqnPx6wsxR"
   },
   "source": [
    "read training file and create "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qdkYbs_rwsxT"
   },
   "outputs": [],
   "source": [
    "def read_file_init_table(fname):\n",
    "    tag_count = {}\n",
    "    tag_count['<start>'] = 0\n",
    "    word_tag = {}\n",
    "    tag_trans = {}\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    idx_line = 0\n",
    "    is_first_word = 0\n",
    "    \n",
    "    while idx_line < len(content):\n",
    "        prev_tag = '<start>'\n",
    "        while not content[idx_line].startswith('</kalimat'):\n",
    "            if  not content[idx_line].startswith('<kalimat'):\n",
    "                content_part = content[idx_line].split('\\t')\n",
    "                if content_part[1] in tag_count:\n",
    "                    tag_count[content_part[1]] += 1\n",
    "                else:\n",
    "                    tag_count[content_part[1]] = 1\n",
    "                    \n",
    "                current_word_tag = content_part[0]+','+content_part[1]\n",
    "                if current_word_tag in word_tag:\n",
    "                    word_tag[current_word_tag] += 1\n",
    "                else:    \n",
    "                    word_tag[current_word_tag] = 1\n",
    "                    \n",
    "                if is_first_word == 1:\n",
    "                    current_tag_trans = '<start>,'+content_part[1]\n",
    "                    is_first_word = 0\n",
    "                else:\n",
    "                    current_tag_trans = prev_tag+','+content_part[1]\n",
    "                    \n",
    "                if current_tag_trans in tag_trans:\n",
    "                    tag_trans[current_tag_trans] += 1\n",
    "                else:\n",
    "                    tag_trans[current_tag_trans] = 1                    \n",
    "                prev_tag = content_part[1]   \n",
    "                \n",
    "            else:\n",
    "                tag_count['<start>'] += 1\n",
    "                is_first_word = 1\n",
    "            idx_line = idx_line + 1\n",
    "        idx_line = idx_line+1 \n",
    "    return tag_count, word_tag, tag_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M4Xa_aXlwsxX",
    "outputId": "695523cb-40e2-43fd-a798-15faa1bf3464"
   },
   "outputs": [],
   "source": [
    "tag_count, word_tag, tag_trans = read_file_init_table('Indonesian_Manually_Tagged_Corpus_ID.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A9i5zqkWwsxc"
   },
   "outputs": [],
   "source": [
    "def create_trans_prob_table(tag_trans, tag_count):\n",
    "    trans_prob = {}\n",
    "    for tag1 in tag_count.keys():\n",
    "        for tag2 in tag_count.keys():\n",
    "            trans_idx = tag1+','+tag2\n",
    "            if trans_idx in tag_trans:\n",
    "                trans_prob[trans_idx] = tag_trans[trans_idx]/tag_count[tag1]\n",
    "    return trans_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqIZemLYwsxg"
   },
   "outputs": [],
   "source": [
    "trans_prob = create_trans_prob_table(tag_trans, tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-WER7wSwsxj"
   },
   "outputs": [],
   "source": [
    "def create_emission_prob_table(word_tag, tag_count):\n",
    "    emission_prob = {}\n",
    "    for word_tag_entry in word_tag.keys():\n",
    "        word_tag_split = re.split(r'[,](?=[a-zA-Z])', word_tag_entry)\n",
    "        current_word = word_tag_split[0]\n",
    "        current_tag = word_tag_split[1]\n",
    "        emission_key = current_word+','+current_tag\n",
    "        emission_prob[emission_key] = word_tag[word_tag_entry]/tag_count[current_tag]    \n",
    "    return emission_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rmv8OFUlwsxm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emission_prob = create_emission_prob_table(word_tag, tag_count) # {'word,tag':chance}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_method(trans_prob, emission_prob, sentence_words):\n",
    "    '''\n",
    "    params format:\n",
    "        trans_prob     : {'given_tag,predict_tag' : probability}\n",
    "        emission_prob  : {'word,tag' : probability}\n",
    "        sentence_words : ['word1', 'word2', 'word3', ...]\n",
    "    '''\n",
    "    tag_sequence = []\n",
    "    # looping untuk setiap kata dalam sentence_words\n",
    "    for word in sentence_words: \n",
    "        # possible_tag diisi dengan semua kemungkinan tag untuk kata (emission_prob)\n",
    "        possible_tag = [(key, value) for key, value in emission_prob.items() if key.startswith(word+',')]\n",
    "        # jika tag terdaftar maka ambil tag dengan kemungkinan tertinggi\n",
    "        if (len(possible_tag) > 0):\n",
    "            max_tag = max(possible_tag,key=itemgetter(1))[0].split(',')[1]\n",
    "        # jika kata tidak terdaftar maka tag dijadikan NNO\n",
    "        else:\n",
    "            max_tag = 'NNO'\n",
    "        # masukkan tag ke dalam tag_sequence\n",
    "        tag_sequence.append(max_tag)\n",
    "    return tag_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRP', 'VB', 'X', 'IN', 'PR']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Ia mengharapkan formalisasi dari tersebut'\n",
    "baseline_method(trans_prob,emission_prob,sentence.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_trans_prob(trans_prob, tag_count):\n",
    "    '''\n",
    "        this function will add all possbile tag trans that are not yet\n",
    "        in tag trans and made it so that the trans prob are 0\n",
    "    '''\n",
    "    new_trans_prob = trans_prob\n",
    "    for tag_a in list(tag_count):\n",
    "        for tag_b in list(tag_count):\n",
    "            if (f'{tag_a},{tag_b}' not in trans_prob):\n",
    "                new_trans_prob[f'{tag_a},{tag_b}'] = 0\n",
    "    return new_trans_prob\n",
    "\n",
    "trans_prob = populate_trans_prob(trans_prob, tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SHXOuNVowsxr"
   },
   "outputs": [],
   "source": [
    "def viterbi(trans_prob, emission_prob, sentence_words):\n",
    "    '''\n",
    "    Params format:\n",
    "        trans_prob     : {'given_tag,predict_tag' : probability}\n",
    "        emission_prob  : {'word,tag' : probability}\n",
    "        sentence_words : ['word1', 'word2', 'word3', ...]\n",
    "    '''\n",
    "    \n",
    "    # assign prev_seq menjadi <start> with the probability of 1\n",
    "    prev_seq = [('<start>',1)]\n",
    "    last_possible_seq = []\n",
    "    \n",
    "    # looping untuk setiap kata dalam sentence_words\n",
    "    for curr_word in sentence_words:\n",
    "        '''\n",
    "        Variable Format:\n",
    "            curr      = [(word, tag)]\n",
    "            c_tag     = (word, tag)\n",
    "            p_seq     = [(prev_squence, prob)]\n",
    "            pre_tag   = previous tag\n",
    "            cur_tag   = current tag\n",
    "        '''\n",
    "        # assign curr_tag with all possible tag for the current word\n",
    "        curr_tag = [(key, value) for key, value in emission_prob.items() if key.startswith(curr_word+',')]\n",
    "        best_seq = []\n",
    "        # looping untuk setiap tag\n",
    "        for c_tag in curr_tag:\n",
    "            possible_seq = {}\n",
    "            # looping untuk semua kemungkinan sequence sebelumnya\n",
    "            for p_seq in prev_seq:\n",
    "                prev_prob = p_seq[1]\n",
    "                # karena p_seq & c_tag disimpan dalam format 'tag1,tag2,tag3'\n",
    "                # maka akan di split by ',' dan diambil index paling terakhirnya\n",
    "                pre_tag = p_seq[0].split(',')[-1]\n",
    "                cur_tag = c_tag[0].split(',')[-1]\n",
    "                # hitung emission dan transition\n",
    "                emission = emission_prob[f'{curr_word},{cur_tag}']\n",
    "                transition = trans_prob[f'{pre_tag},{cur_tag}']\n",
    "                # hitung probability\n",
    "                prob = emission * transition * prev_prob\n",
    "                possible_seq[f'{p_seq[0]},{cur_tag}'] = prob\n",
    "            # ambil sequence dengan probability terbesar dan masukkan ke dalam best_seq\n",
    "            best_key = sorted(possible_seq, key=lambda x:x[1], reverse=True)[0]\n",
    "            best_seq.append((best_key, possible_seq[best_key]))\n",
    "            # apabila kata sekarang adalah kata terakhir, maka langsung mencari sequence terbaik\n",
    "            if (curr_word == sentence_words[-1]):\n",
    "                last_possible_seq.append((best_key, possible_seq[best_key]))\n",
    "        prev_seq = best_seq\n",
    "    # ambil possible sequence yg terbaik\n",
    "    # dan ambil hanya sequencenya saja, tidak dengan probability dan sudah di split by (',')\n",
    "    return sorted(last_possible_seq, key=lambda x:x[1], reverse=True)[0][0].split(',')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CD',\n",
       " 'NN',\n",
       " 'VB',\n",
       " 'PRP',\n",
       " 'RB',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'SC',\n",
       " 'RB',\n",
       " 'CD',\n",
       " 'VB',\n",
       " 'SC',\n",
       " 'NN']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Banyak orang menduga mereka ingin membujuk para tamu agar lebih banyak menyumbang untuk amal'\n",
    "viterbi(trans_prob, emission_prob, sentence.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(fname):\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    idx_line = 0\n",
    "    is_first_word = 0\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    \n",
    "    while idx_line < len(content):\n",
    "        curr_word_list = []\n",
    "        curr_tag_list  = []\n",
    "        while not content[idx_line].startswith('</kalimat'):\n",
    "            if not content[idx_line].startswith('<kalimat'):\n",
    "                split_cont = content[idx_line].split('\\t')\n",
    "                curr_word_list.append(split_cont[0])\n",
    "                curr_tag_list.append(split_cont[1])\n",
    "            idx_line += 1\n",
    "        sentences.append(curr_word_list)\n",
    "        tags.append(curr_tag_list)\n",
    "        idx_line += 1\n",
    "    \n",
    "    return sentences,tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences,tags = read_dataset('Indonesian_Manually_Tagged_Corpus_ID.tsv')\n",
    "\n",
    "sentences,tags = sentences[:1000],tags[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'is_last' : True if index == len(sentence) - 1 else False,\n",
    "        'is_first': True if index == 0 else False,\n",
    "    }\n",
    " \n",
    "def transform_to_dataset(sentences, tags):\n",
    "    X, y = [], []\n",
    " \n",
    "    for sentence_idx in range(len(sentences)):\n",
    "        for index in range(len(sentences[sentence_idx])):\n",
    "            X.append(features(sentences[sentence_idx], index))\n",
    "            y.append(tags[sentence_idx][index])\n",
    " \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train test split with ratio of 3:1\n",
    "cutoff = int(.75 * len(sentences))\n",
    "training_sentences = sentences[:cutoff]\n",
    "test_sentences = sentences[cutoff:]\n",
    "training_tags = tags[:cutoff]\n",
    "test_tags = tags[cutoff:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "X, y = transform_to_dataset(training_sentences, training_tags)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    " \n",
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=10))\n",
    "])\n",
    "\n",
    "clf.fit(X, y)   \n",
    " \n",
    "print('Training completed')\n",
    "\n",
    "X_test, y_test = transform_to_dataset(test_sentences, test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CD' 'NN' 'VB' 'PRP' 'RB' 'VB' 'DT' 'NN' 'SC' 'RB' 'CD' 'VB' 'IN' 'NN']\n"
     ]
    }
   ],
   "source": [
    "# # Test model yang sudah dilatih dengan kalimat masukan bebas\n",
    "\n",
    "def pos_tag(sentence):\n",
    "    tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n",
    "#     return list(zip(sentence, tags))\n",
    "    return tags\n",
    " \n",
    "print(pos_tag('Banyak orang menduga mereka ingin membujuk para tamu agar lebih banyak menyumbang untuk amal'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkAccuracy(y_pred, y_train):\n",
    "    idx = 0\n",
    "    true_predicted_tag = 0\n",
    "    number_of_tag = 0\n",
    "    # looping sebanyak panjang dari data yang di test\n",
    "    while idx < len(y_pred):\n",
    "        # kalau panjang list sama (karena bisa saja tidak)\n",
    "        if len(y_pred[idx]) == len(y_train[idx]):\n",
    "            pred = np.array(y_pred[idx])\n",
    "            true = np.array(y_train[idx])\n",
    "            # hitung jumlah yang sama\n",
    "            true_predicted_tag += sum(pred == true)\n",
    "            # hitung jumlah tag yang dicek\n",
    "            number_of_tag += len(pred)\n",
    "        idx += 1\n",
    "    # kembalikan akurasinya\n",
    "    return true_predicted_tag/number_of_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi_pred = []\n",
    "baseline_pred = []\n",
    "for sent in test_sentences:\n",
    "    viterbi_pred.append(viterbi(trans_prob, emission_prob, sent))\n",
    "    baseline_pred.append(baseline_method(trans_prob,emission_prob,sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from 250 test data:\n",
      "    Baseline      : 0.8681524083393243\n",
      "    Random Forest : 0.9350107836089144\n",
      "    Viterbi       : 0.9529359693124816\n"
     ]
    }
   ],
   "source": [
    "# Accuracy for Baseline\n",
    "\n",
    "print(f'from {len(test_tags)} test data:')\n",
    "print(f'    Baseline      : {checkAccuracy(baseline_pred, test_tags)}')\n",
    "print(f'    Random Forest : {clf.score(X_test, y_test)}')\n",
    "print(f'    Viterbi       : {checkAccuracy(viterbi_pred, test_tags)}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "viterbi_postag_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
